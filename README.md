# ğŸ” Sistema de VerificaciÃ³n de Hechos con RAG

> **Proyecto Final - Procesamiento del Lenguaje Natural**  
> MÃ¡ster en Inteligencia Artificial Aplicada - UC3M  
> Curso 2025/2026

---

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Docker (Recomendado - Todo en Contenedores)

```bash
make all
```

**ğŸ³ Docker se instalarÃ¡ automÃ¡ticamente en Ubuntu 24.04** si no lo tienes.

Ese Ãºnico comando:
- âœ… Verifica e instala Docker si es necesario
- âœ… Ingiere los datos a ChromaDB
- âœ… Construye las imÃ¡genes Docker (backend + frontend)
- âœ… Inicia ambos servicios en contenedores
- âœ… Backend: http://localhost:8000
- âœ… Frontend: http://localhost:5174

**Detener los contenedores:**
```bash
make docker-down
```

**Ver logs:**
```bash
docker logs -f factchecker-backend
docker logs -f factchecker-frontend
```

### OpciÃ³n 2: Desarrollo Local (Sin Docker)

```bash
make dev
```

**IMPORTANTE - Dependencias del Sistema (solo para desarrollo local)**

En Ubuntu/Linux:
```bash
sudo apt update
sudo apt install python3.12 python3.12-venv python3-pip nodejs npm -y
```

En Windows:
- Python 3.12: https://www.python.org/downloads/
- Node.js 18+: https://nodejs.org/

Ese Ãºnico comando:
- âœ… Instala todas las dependencias (Python + Node.js)
- âœ… Descarga modelos necesarios (spaCy)
- âœ… Ingiere los datos a la base vectorial ChromaDB
- âœ… Inicia el backend con hot-reload en http://localhost:8000
- âœ… Inicia el frontend con hot-reload en http://localhost:5174

**ğŸ“Œ Importante**: El frontend se ejecuta en el puerto **5174** (configurado en Vite).

**ğŸ§ Ubuntu 24.04**: Ver [UBUNTU_SETUP.md](UBUNTU_SETUP.md) para troubleshooting detallado.

### OpciÃ³n 3: Windows (sin Make)

Ejecuta el script de inicio:
```bash
.\start.bat
```

O manualmente en dos terminales:

**Terminal 1 - Backend:**
```bash
pip install -r requirements.txt
python -m spacy download es_core_news_sm
python test.py --clear
uvicorn api.server:app --reload --port 8000
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm install
npm run dev
```

---

## ğŸ“‹ DescripciÃ³n General

Sistema inteligente de **verificaciÃ³n automÃ¡tica de hechos** basado en **Retrieval-Augmented Generation (RAG)** que analiza afirmaciones y determina su veracidad utilizando una base documental especÃ­fica sobre equipos de fÃºtbol de la Comunidad de Madrid.

El sistema combina tÃ©cnicas avanzadas de NLP:
- **BÃºsqueda semÃ¡ntica vectorial** con ChromaDB
- **Reranking** con modelos cross-encoder
- **GeneraciÃ³n aumentada** con LLM (Ollama/OpenAI)
- **Procesamiento multilingÃ¼e** con traducciÃ³n automÃ¡tica

### âœ¨ CaracterÃ­sticas Principales

- âœ… **VerificaciÃ³n precisa**: Clasifica afirmaciones como `VERDADERO`, `FALSO` o `NO SE PUEDE VERIFICAR`
- ğŸŒ **Soporte multilingÃ¼e**: Acepta consultas en espaÃ±ol, inglÃ©s, francÃ©s, alemÃ¡n, italiano y portuguÃ©s
- ğŸ“š **Base vectorial ChromaDB**: Almacenamiento y recuperaciÃ³n eficiente con embeddings semÃ¡nticos
- ğŸ¯ **Reranking inteligente**: Mejora la relevancia con modelos de cross-encoder
- ğŸ’¾ **Sistema de cachÃ©**: Optimiza consultas repetidas para respuestas instantÃ¡neas
- ğŸ“– **Citaciones precisas**: Referencias exactas con documento fuente y ubicaciÃ³n
- ğŸ“Š **Nivel de confianza**: PuntuaciÃ³n de certeza del veredicto (0-5 estrellas)
- ğŸ” **Logging detallado**: Registro completo de operaciones para debugging
- ğŸ¨ **Interfaz web moderna**: Frontend React + Tailwind CSS con experiencia de usuario optimizada

## ğŸ¯ Dominio del Conocimiento

El sistema estÃ¡ especializado en verificar hechos sobre los **equipos de fÃºtbol de la Comunidad de Madrid**:

- âšª **Real Madrid CF** - Historia, palmarÃ©s, jugadores
- ğŸ”´ **AtlÃ©tico de Madrid** - Trayectoria, tÃ­tulos, estadio
- ğŸ”µ **Getafe CF** - Logros, historia reciente
- ğŸ’™ **CD LeganÃ©s** - Historia, ascensos y descensos
- âš¡ **Rayo Vallecano** - CaracterÃ­sticas, cultura del club

**Base documental**: 11 archivos de texto con informaciÃ³n extraÃ­da de Wikipedia (actualizada a 2024).

## ğŸ—ï¸ Arquitectura del Sistema

### ğŸ”„ Flujo de VerificaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ENTRADA DEL USUARIO                                     â”‚
â”‚     "El Real Madrid ganÃ³ su primera Champions en 1956"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. PROCESAMIENTO MULTILINGÃœE                               â”‚
â”‚     â€¢ DetecciÃ³n automÃ¡tica de idioma (FastText)            â”‚
â”‚     â€¢ TraducciÃ³n a espaÃ±ol si es necesario                 â”‚
â”‚     â€¢ NormalizaciÃ³n y limpieza de texto                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. RECUPERACIÃ“N SEMÃNTICA (RAG)                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚     â”‚ 3.1 BÃºsqueda Vectorial (ChromaDB)                 â”‚  â”‚
â”‚     â”‚     â€¢ Embeddings: paraphrase-multilingual-mpnet   â”‚  â”‚
â”‚     â”‚     â€¢ Similitud coseno                            â”‚  â”‚
â”‚     â”‚     â€¢ Top-k = 50 documentos candidatos            â”‚  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â”‚                                     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚     â”‚ 3.2 Reranking (Cross-Encoder)                     â”‚  â”‚
â”‚     â”‚     â€¢ Modelo: ms-marco-MiniLM-L-6-v2              â”‚  â”‚
â”‚     â”‚     â€¢ Refinamiento de relevancia                  â”‚  â”‚
â”‚     â”‚     â€¢ Top-k = 5 documentos finales                â”‚  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. GENERACIÃ“N CON LLM                                      â”‚
â”‚     â€¢ Prompt few-shot con ejemplos                         â”‚
â”‚     â€¢ Contexto: Top-5 documentos mÃ¡s relevantes           â”‚
â”‚     â€¢ Razonamiento estructurado                            â”‚
â”‚     â€¢ GeneraciÃ³n JSON con veredicto + explicaciÃ³n         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. RESPUESTA ESTRUCTURADA                                  â”‚
â”‚     {                                                       â”‚
â”‚       "veredicto": "VERDADERO",                            â”‚
â”‚       "explicacion": "El Real Madrid ganÃ³...",             â”‚
â”‚       "confianza": 5,                                       â”‚
â”‚       "fuentes": ["Real_Madrid_Club_de_Futbol.txt"],       â”‚
â”‚       "idioma_original": "es"                              â”‚
â”‚     }                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. TRADUCCIÃ“N DE RESPUESTA (si aplica)                    â”‚
â”‚     â€¢ TraducciÃ³n al idioma original del usuario            â”‚
â”‚     â€¢ PreservaciÃ³n de estructura y fuentes                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§© Componentes del Sistema

#### **1. Frontend (React + TypeScript + Vite)**
- **UbicaciÃ³n**: `frontend/`
- **TecnologÃ­as**: React 18, TypeScript, Tailwind CSS, Vite
- **Puerto**: `http://localhost:5174`
- **Funcionalidades**:
  - Interfaz de chat interactiva
  - Entrada de afirmaciones
  - VisualizaciÃ³n de veredictos con badges dinÃ¡micos
  - Mostrado de fuentes y nivel de confianza
  - DiseÃ±o responsive y moderno

#### **2. Backend (FastAPI)**
- **UbicaciÃ³n**: `api/server.py`
- **Puerto**: `http://localhost:8000`
- **Endpoints**:
  - `POST /verify` - VerificaciÃ³n de afirmaciones
  - `GET /health` - Estado del servicio
  - `GET /stats` - EstadÃ­sticas del sistema
- **CORS**: Configurado para desarrollo local

#### **3. Verificador de Hechos (FactChecker)**
- **UbicaciÃ³n**: `verifier/verifier.py`
- **Funcionalidades**:
  - GestiÃ³n de pipeline completo de verificaciÃ³n
  - IntegraciÃ³n con ChromaDB
  - Reranking con cross-encoder
  - GeneraciÃ³n con LLM (Ollama/OpenAI)
  - Sistema de cachÃ© de consultas
  - Logging detallado

#### **4. Procesador MultilingÃ¼e**
- **UbicaciÃ³n**: `language/pipeline_idioma.py`
- **Idiomas soportados**: ES, EN, FR, DE, IT, PT
- **Funcionalidades**:
  - DetecciÃ³n automÃ¡tica con FastText
  - TraducciÃ³n bidireccional con Deep Translator
  - ValidaciÃ³n de calidad de traducciÃ³n

#### **5. Sistema de RecuperaciÃ³n**
- **UbicaciÃ³n**: `retriever/`
- **Componentes**:
  - `advanced_retriever.py` - Pipeline avanzado con reranking
  - `rag_retriever.py` - RecuperaciÃ³n bÃ¡sica RAG
- **TÃ©cnicas**:
  - BÃºsqueda vectorial semÃ¡ntica
  - Reranking con cross-encoder
  - Filtrado por relevancia

#### **6. Base de Datos Vectorial**
- **TecnologÃ­a**: ChromaDB
- **Embeddings**: `paraphrase-multilingual-mpnet-base-v2` (Sentence Transformers)
- **DimensiÃ³n**: 768
- **Almacenamiento**: `data/vector_store/`

#### **7. Procesamiento de Documentos**
- **UbicaciÃ³n**: `preprocessor/`, `chunker/`
- **Estrategias de chunking**:
  - Chunking semÃ¡ntico con spaCy
  - Chunking hÃ­brido (fijo + semÃ¡ntico)
  - Chunk size: ~500 tokens con overlap de 50
- **Metadatos**: ExtracciÃ³n de entidades, fechas, equipos

#### **8. Ingesta de Datos**
- **Script**: `ingest/ingest_data.py`
- **Proceso**:
  1. Carga documentos desde `data/raw/`
  2. Preprocesamiento y limpieza
  3. Chunking inteligente
  4. GeneraciÃ³n de embeddings
  5. Almacenamiento en ChromaDB

## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

### Requisitos Previos

- **Python**: 3.9 o superior
- **Node.js**: 16 o superior
- **RAM**: 8GB mÃ­nimo (16GB recomendado)
- **Ollama** (opcional): Para usar LLM local
- **OpenAI API Key** (opcional): Para usar GPT-4

### âš¡ EjecuciÃ³n RÃ¡pida con Makefile (Recomendado)

El proyecto incluye un **Makefile** que automatiza todo el proceso:

```bash
# Ejecutar TODO el proyecto (instalar, ingerir, iniciar)
make all
```

Este comando:
1. âœ… Instala todas las dependencias de Python
2. âœ… Descarga el modelo de spaCy (`es_core_news_sm`)
3. âœ… Instala dependencias de Node.js (frontend)
4. âœ… Ingiere los datos a la base vectorial ChromaDB
5. âœ… Inicia el backend (API) en `http://localhost:8000`
6. âœ… Inicia el frontend en `http://localhost:5174`

**Â¡Y listo!** Abre tu navegador en `http://localhost:5174` para usar el sistema.

### ğŸ“š Otros Comandos Ãštiles del Makefile

```bash
# Ver todos los comandos disponibles
make help

# Solo instalar dependencias
make install

# Solo ingerir datos (si ya los tienes instalados)
make ingest

# Iniciar solo el backend
make backend

# Iniciar solo el frontend
make frontend

# Limpiar archivos temporales y cachÃ©
make clean

# Resetear todo (limpiar, instalar e ingerir)
make reset

# Desarrollo sin Docker
make dev
```

### ğŸ³ EjecuciÃ³n con Docker (Alternativa)

```bash
# Construir imagen y levantar contenedores
make docker-build
make docker-up

# En otra terminal, iniciar el frontend
make frontend

# Detener contenedores
make docker-down
```

### ğŸ”§ InstalaciÃ³n Manual (Sin Makefile)

Si prefieres hacerlo paso a paso:

#### 1. Instalar dependencias de Python

```bash
pip install -r requirements.txt
python -m spacy download es_core_news_sm
```

#### 2. Instalar dependencias del frontend

```bash
cd frontend
npm install
cd ..
```

#### 3. Ingerir datos a ChromaDB

```bash
python test.py --clear
```

#### 4. Iniciar el backend (en una terminal)

```bash
uvicorn api.server:app --reload --port 8000
```

#### 5. Iniciar el frontend (en otra terminal)

```bash
cd frontend
npm run dev
```

#### 6. Abrir en el navegador

Accede a: `http://localhost:5174`

## ğŸ’¡ ExplicaciÃ³n del CÃ³digo

### Componentes Clave

#### 1. **FactChecker** (`verifier/verifier.py`)

Clase principal que orquesta todo el pipeline de verificaciÃ³n:

```python
class FactChecker:
    def __init__(self, config_path: str = "config.yaml"):
        # Carga configuraciÃ³n desde YAML
        self.config = ConfigManager(config_path)
        
        # Inicializa procesador multilingÃ¼e
        self.linguist = PipelineIdioma()
        
        # Carga modelo de embeddings para bÃºsqueda semÃ¡ntica
        self.embeddings = HuggingFaceEmbeddings(
            model_name="paraphrase-multilingual-mpnet-base-v2"
        )
        
        # Conecta a la base vectorial ChromaDB
        self.vector_db = Chroma(
            persist_directory="data/vector_store",
            embedding_function=self.embeddings
        )
        
        # Inicializa modelo de reranking
        self.reranker = CrossEncoder(
            "cross-encoder/ms-marco-MiniLM-L-6-v2"
        )
        
        # Inicializa LLM (Ollama o OpenAI)
        self.llm = ChatOllama(model="llama3.1", temperature=0.0)
        
    def verify(self, claim_usuario: str) -> Dict[str, Any]:
        """Pipeline completo de verificaciÃ³n"""
        # 1. Detectar idioma y traducir si es necesario
        idioma = self.linguist.detect_language(claim_usuario)
        claim_es = self.linguist.translate(claim_usuario, to_lang="es")
        
        # 2. BÃºsqueda semÃ¡ntica en ChromaDB (top-50)
        docs = self.vector_db.similarity_search(claim_es, k=50)
        
        # 3. Reranking con cross-encoder (top-5)
        docs_reranked = self.reranker.rank(claim_es, docs, top_k=5)
        
        # 4. Construir contexto
        context = "\n\n".join([doc.page_content for doc in docs_reranked])
        
        # 5. Generar prompt few-shot
        prompt = self.prompts.format(claim=claim_es, context=context)
        
        # 6. Invocar LLM para obtener veredicto
        response = self.llm.invoke(prompt)
        resultado = json.loads(response.content)
        
        # 7. Traducir respuesta al idioma original
        if idioma != "es":
            resultado["explicacion"] = self.linguist.translate(
                resultado["explicacion"], to_lang=idioma
            )
        
        return resultado
```

**Flujo paso a paso**:
1. **DetecciÃ³n de idioma**: FastText identifica el idioma de entrada
2. **TraducciÃ³n**: Si no es espaÃ±ol, traduce a espaÃ±ol para bÃºsqueda
3. **BÃºsqueda vectorial**: ChromaDB recupera los 50 chunks mÃ¡s similares
4. **Reranking**: Cross-encoder refina a los 5 mÃ¡s relevantes
5. **ConstrucciÃ³n de contexto**: Concatena los documentos seleccionados
6. **GeneraciÃ³n LLM**: Llama al modelo con prompt few-shot
7. **Post-procesamiento**: Traduce la respuesta al idioma original

#### 2. **API FastAPI** (`api/server.py`)

Servidor HTTP que expone el verificador:

```python
from fastapi import FastAPI
from verifier import FactChecker

app = FastAPI(title="FactChecker API")
fact_checker = FactChecker()  # Instancia global reutilizable

@app.post("/verify")
async def verify_claim(request: VerifyRequest):
    """Endpoint principal de verificaciÃ³n"""
    result = fact_checker.verify(request.question)
    return {
        "verdict": _map_verdict_tag(result["veredicto"]),
        "explanation": result["explicacion_corta"],
        "confidence": result["nivel_confianza"],
        "sources": result.get("fuentes", []),
        "language": result.get("idioma_respuesta", "es")
    }
```

**CaracterÃ­sticas**:
- CORS configurado para permitir frontend en localhost:5174
- Instancia global de FactChecker (evita recargar modelos en cada request)
- TransformaciÃ³n de respuesta a formato JSON estÃ¡ndar

#### 3. **Ingesta de Datos** (`ingest/ingest_data.py`)

Script que prepara la base vectorial:

```python
def main():
    # 1. Cargar documentos desde data/raw/
    loader = DirectoryLoader("data/raw/", glob="**/*.txt")
    documents = loader.load()
    
    # 2. Preprocesar (limpiar, normalizar)
    preprocessor = DocumentPreprocessor()
    docs_clean = [preprocessor.clean(doc) for doc in documents]
    
    # 3. Chunking semÃ¡ntico con spaCy
    chunker = SemanticChunker(chunk_size=500, chunk_overlap=50)
    chunks = chunker.split_documents(docs_clean)
    
    # 4. Extraer metadatos (equipos, fechas, entidades)
    extractor = MetadataExtractor()
    chunks_with_metadata = [
        extractor.extract(chunk) for chunk in chunks
    ]
    
    # 5. Generar embeddings y guardar en ChromaDB
    embeddings = HuggingFaceEmbeddings(model_name="...")
    db = Chroma.from_documents(
        documents=chunks_with_metadata,
        embedding=embeddings,
        persist_directory="data/vector_store/"
    )
    db.persist()
```

**Proceso**:
1. **Carga**: Lee todos los archivos .txt de `data/raw/`
2. **Preprocesamiento**: Elimina caracteres especiales, normaliza espacios
3. **Chunking**: Divide en fragmentos semÃ¡nticamente coherentes (~500 tokens)
4. **Metadatos**: Extrae equipos mencionados, fechas, nombres propios
5. **Embeddings**: Genera vectores de 768 dimensiones con Sentence Transformers
6. **Persistencia**: Guarda todo en ChromaDB para bÃºsqueda rÃ¡pida

#### 4. **Pipeline MultilingÃ¼e** (`language/pipeline_idioma.py`)

Gestiona detecciÃ³n y traducciÃ³n:

```python
class PipelineIdioma:
    SUPPORTED_LANGS = ["es", "en", "fr", "de", "it", "pt"]
    
    def __init__(self):
        # Cargar modelo FastText para detecciÃ³n
        self.detector = fasttext.load_model("data/lid.176.ftz")
        
    def detect_language(self, text: str) -> str:
        """Detecta idioma con FastText"""
        predictions = self.detector.predict(text, k=1)
        lang_code = predictions[0][0].replace("__label__", "")
        return lang_code if lang_code in self.SUPPORTED_LANGS else "es"
    
    def translate(self, text: str, to_lang: str) -> str:
        """Traduce con Google Translator"""
        translator = GoogleTranslator(source='auto', target=to_lang)
        return translator.translate(text)
```

**CaracterÃ­sticas**:
- FastText: 176 idiomas soportados, detecciÃ³n ultrarrÃ¡pida
- Google Translator: TraducciÃ³n de alta calidad
- ValidaciÃ³n: Solo acepta idiomas en whitelist

#### 5. **Frontend React** (`frontend/src/`)

Interfaz web moderna:

```typescript
// Componente principal de verificaciÃ³n
function VerifyForm() {
  const [claim, setClaim] = useState("");
  const [result, setResult] = useState(null);
  
  const handleSubmit = async () => {
    const response = await fetch("http://localhost:8000/verify", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ question: claim })
    });
    
    const data = await response.json();
    setResult(data);
  };
  
  return (
    <div className="max-w-4xl mx-auto p-6">
      <textarea 
        value={claim}
        onChange={(e) => setClaim(e.target.value)}
        placeholder="Escribe una afirmaciÃ³n..."
      />
      <button onClick={handleSubmit}>Verificar</button>
      
      {result && (
        <ResultCard 
          verdict={result.verdict}
          explanation={result.explanation}
          sources={result.sources}
          confidence={result.confidence}
        />
      )}
    </div>
  );
}
```

**TecnologÃ­as**:
- **React 18**: Componentes funcionales con hooks
- **TypeScript**: Tipado estÃ¡tico para robustez
- **Tailwind CSS**: Estilos utility-first
- **Vite**: Build tool ultrarrÃ¡pido

### Archivo de ConfiguraciÃ³n (`config.yaml`)

```yaml
# Modelos
embeddings:
  model_name: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  device: "cpu"  # o "cuda" si tienes GPU

llm:
  provider: "ollama"  # o "openai"
  model: "llama3.1"
  temperature: 0.0    # Determinista
  max_tokens: 2000

# RAG
retriever:
  k: 50              # BÃºsqueda inicial
  top_k: 5           # DespuÃ©s de reranking
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# Chunking
chunker:
  strategy: "semantic"  # o "hybrid", "fixed"
  chunk_size: 500
  chunk_overlap: 50

# Logging
logging:
  level: "INFO"
  file: "logs/factchecker.log"
```

Este archivo centraliza toda la configuraciÃ³n, permitiendo cambiar modelos y parÃ¡metros sin modificar cÃ³digo.

## ğŸ“¦ Estructura del Proyecto

```
NLP-VERIFICAR_DE_HECHOS-v3/
â”‚
â”œâ”€â”€ ğŸ“„ Makefile                    # AutomatizaciÃ³n de tareas
â”œâ”€â”€ ğŸ“„ Dockerfile                  # Contenedor Docker
â”œâ”€â”€ ğŸ“„ requirements.txt            # Dependencias Python
â”œâ”€â”€ ğŸ“„ config.yaml                 # ConfiguraciÃ³n global
â”œâ”€â”€ ğŸ“„ README.md                   # Este archivo
â”‚
â”œâ”€â”€ ğŸ“ api/                        # API REST FastAPI
â”‚   â””â”€â”€ server.py                  # Servidor HTTP
â”‚
â”œâ”€â”€ ğŸ“ verifier/                   # Motor de verificaciÃ³n
â”‚   â”œâ”€â”€ verifier.py                # FactChecker principal
â”‚   â””â”€â”€ prompts.py                 # Templates de prompts
â”‚
â”œâ”€â”€ ğŸ“ retriever/                  # Sistema de recuperaciÃ³n
â”‚   â”œâ”€â”€ advanced_retriever.py     # Pipeline con reranking
â”‚   â”œâ”€â”€ rag_retriever.py           # RAG bÃ¡sico
â”‚   â””â”€â”€ hyde_retriever.py          # HyDE (Hypothetical Doc Embeddings)
â”‚
â”œâ”€â”€ ğŸ“ language/                   # Procesamiento multilingÃ¼e
â”‚   â””â”€â”€ pipeline_idioma.py         # DetecciÃ³n y traducciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ preprocessor/               # Preprocesamiento de textos
â”‚   â””â”€â”€ document_preprocessor.py  # Limpieza y normalizaciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ chunker/                    # Estrategias de chunking
â”‚   â”œâ”€â”€ semantic_chunker.py        # Chunking semÃ¡ntico
â”‚   â”œâ”€â”€ hybrid_chunker.py          # Chunking hÃ­brido
â”‚   â””â”€â”€ section_aware.py           # Consciente de secciones
â”‚
â”œâ”€â”€ ğŸ“ extractor/                  # ExtracciÃ³n de metadatos
â”‚   â”œâ”€â”€ metadata_extractor.py     # Metadatos generales
â”‚   â”œâ”€â”€ fact_metadata_extractor.py # Metadatos de hechos
â”‚   â””â”€â”€ topic_extractor.py         # ExtracciÃ³n de tÃ³picos
â”‚
â”œâ”€â”€ ğŸ“ ingest/                     # Ingesta de documentos
â”‚   â””â”€â”€ ingest_data.py             # Pipeline de ingesta
â”‚
â”œâ”€â”€ ğŸ“ frontend/                   # Interfaz web
â”‚   â”œâ”€â”€ src/                       # CÃ³digo fuente React
â”‚   â”œâ”€â”€ package.json               # Dependencias Node.js
â”‚   â”œâ”€â”€ vite.config.ts             # ConfiguraciÃ³n Vite
â”‚   â””â”€â”€ tailwind.config.js         # ConfiguraciÃ³n Tailwind
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Datos del sistema
â”‚   â”œâ”€â”€ raw/                       # Documentos originales (11 archivos .txt)
â”‚   â”œâ”€â”€ vector_store/              # Base vectorial ChromaDB
â”‚   â””â”€â”€ lid.176.ftz                # Modelo FastText para detecciÃ³n de idioma
â”‚
â”œâ”€â”€ ğŸ“ logs/                       # Logs del sistema
â”œâ”€â”€ ğŸ“ evaluations/                # Resultados de evaluaciones
â””â”€â”€ ğŸ“ test_parts/                 # Tests unitarios
```

## ğŸ® Uso del Sistema

### ğŸ’¬ Interfaz Web

1. Accede a `http://localhost:5174`
2. Escribe una afirmaciÃ³n en el campo de texto
3. Presiona Enter o haz clic en "Verificar"
4. Observa el veredicto, explicaciÃ³n y fuentes

**Ejemplos de afirmaciones validadas** (85.7% de precisiÃ³n en evaluaciÃ³n):

```
âœ… VERDADERO - Casos verificados correctamente:
- "El Real Madrid fue fundado en 1902"
- "El estadio del Real Madrid se llama Santiago BernabÃ©u"
- "El Real Madrid ha ganado 15 Copas de Europa"
- "El AtlÃ©tico de Madrid ganÃ³ la Liga en la temporada 2020-21"
- "El Getafe CF juega en el Coliseum Alfonso PÃ©rez"
- "El CD LeganÃ©s fue fundado en 1928"
- "El Rayo Vallecano juega en Vallecas"

âŒ FALSO - DetecciÃ³n de falsedades:
- "El Real Madrid fue fundado en 1947" (Fundado en 1902, no 1947)
- "El AtlÃ©tico de Madrid nunca ha ganado la Liga" (Ha ganado 11 veces)
- "El CD LeganÃ©s fue fundado en 1900" (Fundado en 1928, no 1900)

ğŸ” NO VERIFICABLE - Predicciones y afirmaciones fuera de alcance:
- "El Real Madrid ganarÃ¡ la Champions League en 2025" (PredicciÃ³n futura)
- "Messi es el mejor jugador de la historia" (OpiniÃ³n subjetiva)
- "El Barcelona es el mejor equipo de EspaÃ±a" (Fuera del corpus de Madrid)
```

### ğŸ”Œ API REST

#### **Verificar una afirmaciÃ³n**

```bash
curl -X POST "http://localhost:8000/api/verify" \
  -H "Content-Type: application/json" \
  -d '{"question": "El Real Madrid fue fundado en 1902"}'
```

**Respuesta**:

```json
{
  "verdict": "true",
  "explanation": "La afirmaciÃ³n es VERDADERA. Confirma la fundaciÃ³n en 1902. El Real Madrid fue registrado oficialmente como club de fÃºtbol el 6 de marzo de 1902 en una Junta General Extraordinaria.",
  "confidence": 3,
  "sources": [
    {
      "document": "Historia_del_Real_Madrid_Club_de_FÃºtbol.txt",
      "snippet": "legalizaron oficialmente la nueva asociaciÃ³n el 6 de marzo de 1902 en una Junta General Extraordinaria"
    }
  ],
  "language": "es",
  "retrieval_time_ms": 234,
  "llm_time_ms": 6722
}
```

#### **Estado del servicio**

```bash
curl http://localhost:8000/health
```

#### **EstadÃ­sticas**

```bash
curl http://localhost:8000/stats
```

### ğŸ Uso ProgramÃ¡tico (Python)

Puedes usar el verificador directamente en tus scripts Python:

```python
from verifier.verifier import FactChecker

# Inicializar verificador (carga modelos una sola vez)
fact_checker = FactChecker()

# Ejemplos validados (85.7% de precisiÃ³n)
test_cases = [
    "El Real Madrid fue fundado en 1902",
    "El AtlÃ©tico de Madrid ganÃ³ la Liga en la temporada 2020-21",
    "El Getafe CF juega en el Coliseum Alfonso PÃ©rez",
    "El CD LeganÃ©s fue fundado en 1928",
    "El Rayo Vallecano juega en Vallecas",
]

for claim in test_cases:
    result = fact_checker.verify(claim)
    print(f"AfirmaciÃ³n: {claim}")
    print(f"Veredicto: {result['veredicto']}")
    print(f"Confianza: {result['nivel_confianza']}/5")
    print(f"ExplicaciÃ³n: {result['explicacion_corta']}")
    print(f"Fuente: {result['fuente_documento']}")
    print("-" * 80)
```

**Salida esperada**:

```
AfirmaciÃ³n: El Real Madrid fue fundado en 1902
Veredicto: VERDADERO
Confianza: 3/5
ExplicaciÃ³n: Confirma la fundaciÃ³n en 1902
Fuente: Historia_del_Real_Madrid_Club_de_FÃºtbol.txt
--------------------------------------------------------------------------------
AfirmaciÃ³n: El AtlÃ©tico de Madrid ganÃ³ la Liga en la temporada 2020-21
Veredicto: VERDADERO
Confianza: 4/5
ExplicaciÃ³n: Confirma que ganÃ³ la Liga en 2020-21
Fuente: Anexo-PalmarÃ©s_del_Club_AtlÃ©tico_de_Madrid.txt
--------------------------------------------------------------------------------
...
```

## âš™ï¸ ConfiguraciÃ³n

### Archivo `config.yaml`

El sistema se configura mediante `config.yaml`:

```yaml
# Modelo de embeddings
embeddings:
  model_name: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  device: "cpu"  # o "cuda" si tienes GPU

# LLM (Ollama o OpenAI)
llm:
  provider: "ollama"  # o "openai"
  model: "llama3.1"   # o "gpt-4"
  temperature: 0.0
  max_tokens: 2000

# RecuperaciÃ³n
retriever:
  k: 50              # Documentos iniciales
  top_k: 5           # Documentos finales tras reranking
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# Chunking
chunker:
  strategy: "semantic"  # o "hybrid", "fixed"
  chunk_size: 500
  chunk_overlap: 50

# Logging
logging:
  level: "INFO"
  file: "logs/factchecker.log"
```

### Variables de Entorno

Para usar OpenAI:

```bash
# Windows PowerShell
$env:OPENAI_API_KEY = "sk-..."

# Linux/Mac
export OPENAI_API_KEY="sk-..."
```

## ğŸ§ª Testing y EvaluaciÃ³n

### Ejecutar tests

```bash
# Test completo del verificador
python test_verifier_simple.py

# Test de embeddings
python test_embedding_upgrade.py

# Test de recuperaciÃ³n
python test_retrieval_debug.py

# Test de confianza semÃ¡ntica
python test_confidence.py
```

### EvaluaciÃ³n con mÃ©tricas

```bash
python evaluate.py
```

MÃ©tricas implementadas:
- **Exactitud**: % de veredictos correctos
- **PrecisiÃ³n/Recall**: Por clase (VERDADERO/FALSO/NO VERIFICABLE)
- **BERT Score**: Similitud semÃ¡ntica de explicaciones
- **ROUGE**: Calidad de generaciÃ³n de texto
- **Latencia**: Tiempo de respuesta

## ğŸ“Š Requisitos Funcionales Cumplidos

SegÃºn el enunciado del proyecto, el sistema cumple con:

### âœ… Requisitos Obligatorios

1. **Respuesta clara sobre veracidad**  
   âœ“ Veredictos explÃ­citos: `VERDADERO`, `FALSO`, `NO SE PUEDE VERIFICAR`

2. **CitaciÃ³n de fuentes precisas**  
   âœ“ Nombre del documento fuente  
   âœ“ UbicaciÃ³n especÃ­fica (secciÃ³n/pÃ¡gina)  
   âœ“ Snippet de evidencia citado

3. **Manejo de informaciÃ³n insuficiente**  
   âœ“ Responde "NO SE PUEDE VERIFICAR" cuando no hay evidencia  
   âœ“ No inventa informaciÃ³n

4. **Respuesta en idioma original**  
   âœ“ DetecciÃ³n automÃ¡tica del idioma de entrada  
   âœ“ TraducciÃ³n de respuesta al idioma detectado

5. **Base documental especÃ­fica**  
   âœ“ 11 documentos sobre equipos de fÃºtbol de Madrid  
   âœ“ InformaciÃ³n actualizada (Wikipedia 2024)

### âœ¨ CaracterÃ­sticas Adicionales

- ğŸ”„ **Reranking** con cross-encoder para mejor precisiÃ³n
- ğŸ’¾ **Sistema de cachÃ©** para optimizaciÃ³n
- ğŸ“Š **Nivel de confianza** cuantificado (0-5)
- ğŸ¯ **Chunking semÃ¡ntico** inteligente
- ğŸŒ **6 idiomas** soportados
- ğŸ¨ **Interfaz web** moderna y responsive
- ğŸ” **Logging detallado** para debugging
- ğŸ“ˆ **MÃ©tricas de evaluaciÃ³n** completas

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Backend
- **Python 3.11** - Lenguaje principal
- **FastAPI** - Framework web
- **LangChain** - OrquestaciÃ³n RAG
- **ChromaDB** - Base de datos vectorial
- **Sentence Transformers** - Embeddings
- **Ollama / OpenAI** - Modelos LLM
- **spaCy** - NLP y chunking semÃ¡ntico
- **FastText** - DetecciÃ³n de idioma
- **Deep Translator** - TraducciÃ³n

### Frontend
- **React 18** - Biblioteca UI
- **TypeScript** - Tipado estÃ¡tico
- **Vite** - Build tool
- **Tailwind CSS** - Estilos

### DevOps
- **Docker** - Contenedores
- **Make** - AutomatizaciÃ³n
- **Git** - Control de versiones

## ğŸ“ Logging y Debugging

El sistema genera logs detallados en `logs/`:

```
logs/
â”œâ”€â”€ factchecker.log       # Log principal
â”œâ”€â”€ retriever.log         # Logs de recuperaciÃ³n
â”œâ”€â”€ llm.log               # Logs del LLM
â””â”€â”€ api.log               # Logs de la API
```

Nivel de detalle configurable en `config.yaml`:
- `DEBUG`: Todo el detalle (desarrollo)
- `INFO`: InformaciÃ³n general (recomendado)
- `WARNING`: Solo advertencias
- `ERROR`: Solo errores

## ğŸ› SoluciÃ³n de Problemas

### El sistema no encuentra documentos

```bash
# Re-ingestar la base de datos
python test.py --clear
```

### Error de memoria

- Reducir `retriever.k` en `config.yaml`
- Usar embeddings mÃ¡s pequeÃ±os
- Aumentar RAM disponible

### Frontend no conecta con backend

- Verificar que el backend estÃ© en `http://localhost:8000`
- Revisar CORS en `api/server.py`
- Verificar puertos en uso

### Frontend no inicia en Ubuntu (vite: Permission denied)

```bash
# SoluciÃ³n: Dar permisos al ejecutable de vite
cd frontend
chmod +x node_modules/.bin/vite
npm run dev
```

**Alternativa**: Reinstalar dependencias del frontend
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install
npm run dev
```

### Ollama no responde

```bash
# Verificar que Ollama estÃ© corriendo
ollama list

# Iniciar Ollama
ollama serve

# Descargar modelo
ollama pull llama3.1
```

## ğŸ“š Referencias y DocumentaciÃ³n

- **LangChain**: https://python.langchain.com/
- **ChromaDB**: https://docs.trychroma.com/
- **Sentence Transformers**: https://www.sbert.net/
- **FastAPI**: https://fastapi.tiangolo.com/
- **React**: https://react.dev/
- **Ollama**: https://ollama.ai/

## ğŸ‘¥ Autores

**Proyecto Final - MÃ¡ster IA Aplicada UC3M**  
Curso 2024/2025

## ğŸ“„ Licencia

Este proyecto es con fines acadÃ©micos.

---

**ğŸ¯ Â¡Listo para usar!** Ejecuta `make all` y comienza a verificar hechos.

